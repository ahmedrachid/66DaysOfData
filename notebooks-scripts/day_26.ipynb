{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day26 - NLP\n",
    "\n",
    "**Latent Semantic Analysis** (LSA) is based on the well known Singular Value Decomposition (SVD). It is possible to truncate the tf-idf matrix in order to drastically reduce the dimension of the problem. This new representation highlights the \"latent sentiment\" of these topics.\n",
    "\n",
    "So, the LSA tells you which dimensions are relevant to the semantic of the documents. In fact the \"low variance\" topics may represents just noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from nltk import word_tokenize, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/abcnews-date-text.csv')\n",
    "\n",
    "reindexed_data = df['headline_text']\n",
    "reindexed_data.index = df['publish_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    :param data: series of news headlines\n",
    "    :return: preprocessed series of news headlines\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = SnowballStemmer(language=\"english\")\n",
    "    \n",
    "    prep_data = []\n",
    "    \n",
    "    for document in data:\n",
    "        \n",
    "        # lower case normalization\n",
    "        prep_document = document.lower()\n",
    "        \n",
    "        # remove numbers and hashtags\n",
    "        prep_document = re.sub(r\"[0-9]\",\"\",prep_document)\n",
    "        prep_document = re.sub(r\"#\", \"\", prep_document)\n",
    "        \n",
    "        # tokenization\n",
    "        tokens = word_tokenize(prep_document)\n",
    "        \n",
    "        stemmed_tokens = [stemmer.stem(t) for t in tokens if t not in stopwords.words('english')]\n",
    "        \n",
    "        prep_data.append(' '.join(stemmed_tokens))\n",
    "        \n",
    "    return prep_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preprocessed_docs = preprocess_data(reindexed_data.sample(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tf - idf\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_model = TruncatedSVD(n_components=7)\n",
    "lsa_topic_matrix = lsa_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rcushen/topic-modelling-with-lsa-and-lda\n",
    "    \n",
    "def get_keys(topic_matrix):\n",
    "    '''\n",
    "    returns an integer list of predicted topic \n",
    "    categories for a given topic matrix\n",
    "    '''\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n",
    "\n",
    "def keys_to_counts(keys):\n",
    "    '''\n",
    "    returns a tuple of topic categories and their \n",
    "    accompanying magnitudes for a given list of keys\n",
    "    '''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    return categories\n",
    "\n",
    "def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n",
    "    '''\n",
    "    returns a list of n_topic strings, where each string contains the n most common \n",
    "    words in a predicted category, in order\n",
    "    '''\n",
    "    top_word_indices = []\n",
    "    for topic in range(7):\n",
    "        temp_vector_sum = 0\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:,index] = 1\n",
    "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
    "        top_words.append(\" \".join(topic_words))         \n",
    "    return top_words\n",
    "\n",
    "lsa_keys = get_keys(lsa_topic_matrix)\n",
    "lsa_categories = keys_to_counts(lsa_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:  man charg say plan council govt court fire call back\n",
      "Topic 2:  torchin drugsrisk furri fuselag fusion fuss fussi futcher futil futsal\n",
      "Topic 3:  polic investig probe search offic hunt death drug miss arrest\n",
      "Topic 4:  market nation abc rural news busi weather countri hour sport\n",
      "Topic 5:  new zealand law year case appoint york get plan open\n",
      "Topic 6:  australia kill crash car die day world australian year cup\n",
      "Topic 7:  interview report extend win smith nrl michael john us david\n"
     ]
    }
   ],
   "source": [
    "top_n_words_lsa = get_top_n_words(10, lsa_keys, X, vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lsa)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
