{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day22 - NLP\n",
    "\n",
    "**Tokenization** is the process by which the initial string is splitted into smaller units (called tokens). These tokens can be words, digits or punctuation. The current structure allows to analyze each token separately, in order to decide which one can be maintained or not. It can be easily done with _word_tokenize_, by NLTK (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', ',', 'my', 'name', 'is', 'Francesco', '!', ':', ')']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(\"Hi, my name is Francesco! :)\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During any kind of analysis, we're not interested in the most common words, so we can avoid the **stopwords** (aka the most frequent words) as \"I,me,you..\". \n",
    "So, we can filter the stopwords and punctuation just iterating the tokens obtained so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'name', 'Francesco']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "cleaned_tokens = []\n",
    "for token in tokens:\n",
    "    # remove stopwords and punctuation\n",
    "    if token not in stopwords.words('english') and \\\n",
    "       token not in string.punctuation:             \n",
    "        cleaned_tokens.append(token)\n",
    "        \n",
    "cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove some elements thanks to some **regular expressions** (re). In the following example we'll remove digits and \"#\" (useful if we're analyzing tweets et simila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today', 'I', 'feel', 'gr', '!', 'GGWP']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # regular expressions \n",
    "text_with_digits = \"Today I feel gr8! #GGWP\"\n",
    "\n",
    "cleaned_text = re.sub(r\"[0-9]\",\"\",text_with_digits)\n",
    "cleaned_text = re.sub(r\"#\", \"\", cleaned_text)\n",
    "word_tokenize(cleaned_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
